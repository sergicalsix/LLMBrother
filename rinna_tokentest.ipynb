{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67816767-aa51-4d36-98a3-6fc290644226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Any, Dict, Optional, Union\n",
    "import os\n",
    "\n",
    "class LanguageModelManager:\n",
    "    \"\"\"\n",
    "    Hugging FaceのTransformersライブラリを使用して、自然言語生成（NLG）を簡単に行うためのラッパークラス。\n",
    "    \n",
    "    前提条件:\n",
    "        - PyTorchがインストールされていること。\n",
    "        - Hugging FaceのTransformersライブラリがインストールされていること。\n",
    "    \n",
    "    属性:\n",
    "        tokenizer: Hugging FaceのAutoTokenizerインスタンス。\n",
    "        model: Hugging FaceのAutoModelForCausalLMインスタンス。\n",
    "    \n",
    "    参考文献:\n",
    "        - Tokenizerの詳細: https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "        - Modelの詳細: https://huggingface.co/docs/transformers/v4.33.0/en/main_classes/model\n",
    "        \n",
    "    \n",
    "    使用例:\n",
    "        >>> my_model = LanguageModelManager(model_name_or_path = \"rinna/japanese-gpt-1b\")\n",
    "        >>> my_model.generate_text(\"こんにちは\", only_answer=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name_or_path: Union[str, os.PathLike], \n",
    "                 tokenizer_options: Optional[Dict[str, Any]] = None, \n",
    "                 model_options: Optional[Dict[str, Any]] = None) -> None:\n",
    "        \"\"\"\n",
    "        コンストラクタで指定されたモデル名を使用して、tokenizerとmodelを初期化します。\n",
    "        \n",
    "        引数:\n",
    "            model_name (str): 使用するモデルの名前。\n",
    "            tokenizer_option (Optional[Dict[str, Any]]): Tokenizerの設定オプション。\n",
    "            model_option (Optional[Dict[str, Any]]): モデルの設定オプション。\n",
    "        \n",
    "        戻り値:\n",
    "            なし\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, **(tokenizer_options or {}))\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name_or_path, **(model_options or {}))\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.to(\"cuda\")\n",
    "\n",
    "    def generate_text(self, \n",
    "               text: str, \n",
    "               encode_options: Optional[Dict[str, Any]] = None, \n",
    "               generate_options: Optional[Dict[str, Any]] = None, \n",
    "               decode_options: Optional[Dict[str, Any]] = None,\n",
    "               only_answer: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        テキストを入力として受け取り、モデルによって生成されたテキストを返します。\n",
    "        \n",
    "        処理のステップ:\n",
    "            1. テキストをトークンにエンコード。\n",
    "            2. トークンを使用してテキストを生成。\n",
    "            3. 生成されたトークンをデコードしてテキストに変換。\n",
    "        \n",
    "        引数:\n",
    "            text (str): 入力テキスト。\n",
    "            encode_option (Optional[Dict[str, Any]]): エンコードオプション。\n",
    "            generate_option (Optional[Dict[str, Any]]): テキスト生成オプション。\n",
    "            decode_option (Optional[Dict[str, Any]]): デコードオプション。\n",
    "            only_answer (bool): Trueの場合、生成されたテキストのうち、入力テキスト以降の部分のみを返します。\n",
    "        \n",
    "        戻り値:\n",
    "            str: 生成されたテキスト。\n",
    "        \"\"\"\n",
    "        \n",
    "        encode_options = encode_options or {\"return_tensors\": \"pt\"}\n",
    "        \n",
    "        token_ids = self.tokenizer.encode(text, **encode_options)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                input_ids=token_ids.to(self.model.device),\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                bos_token_id=self.tokenizer.bos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                bad_words_ids=[[self.tokenizer.unk_token_id]],\n",
    "                **(generate_option or {})\n",
    "            )\n",
    "        \n",
    "        output = self.tokenizer.decode(output_ids.tolist()[0], **(decode_options or {}))\n",
    "        output = output.replace(\"</s>\", \"\")\n",
    "        \n",
    "        if only_answer:\n",
    "            return output[len(text):]\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ae1bf53-5c36-4150-bf8f-6609b7b5d98f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d96192b-f38c-4293-944d-d53ce9bc3a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'それについては、私が得意とする分野だからなのか、今もなお熱く語ることができるのですが、仕事となると話は別です。どんな人が相手でも、それは仕事です。もちろん、クライアントが要望をおっしゃってくださることは素晴らしいことです。'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LLM(model_name = \"rinna/japanese-gpt-1b\")\n",
    "generate_option = {\"min_length\":50,\"max_length\":50, \"do_sample\": True, \"top_k\": 500,  \"top_p\":0.95}\n",
    "m.output(text = '私の趣味は', generate_option= generate_option, only_answer=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de1d7cad-181c-41bf-a62d-ddfc42a355c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|█████████████████████████████████████████████████████| 360/360 [00:00<00:00, 349kB/s]\n",
      "Downloading spiece.model: 100%|███████████████████████████████████████████████████████████| 1.21M/1.21M [00:01<00:00, 1.18MB/s]\n",
      "/Users/shibuya/python_env/venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████████| 2.01k/2.01k [00:00<00:00, 4.85MB/s]\n",
      "Downloading model.safetensors: 100%|██████████████████████████████████████████████████████| 3.51G/3.51G [10:08<00:00, 5.76MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'趣味は?</s> ウェブ<0x0A>本文:  1000円札を1枚、'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LLM(model_name = \"line-corporation/japanese-large-lm-1.7b\")\n",
    "m.output(\"趣味は？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63cf238a-b95f-44c1-8054-a1d7926deec9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'こんにちは このたび、東洋・東南アジア全域を対象とする統一的な診療指針の策定に向けた政府間協議に東洋医学界を代表して派遣されましたことを謹んでご報告いたします。各国保健省、衛生省との意見交換や関連資料の調査に協力しながら、'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LanguageModelManager(model_name_or_path = \"rinna/japanese-gpt-1b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37de4781-35aa-4baf-98b1-d2bcff7cf3b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' このたび、平成25年1月3日より、表参道ヒルズ (東京都港区南青山)にて、 「COCHA」の新プロモーションイベントを開催いたします。 今回、プロジェクトテーマを「#Bararaness... 続きを読む →...'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.generate_text(text=\"こんにちは\", only_answer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1981bb-ccf4-4dde-8098-cae8d4f2b439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
